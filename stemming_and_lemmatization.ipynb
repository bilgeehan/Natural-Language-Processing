{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPX/YbVkFHZrC38bHwO/sFn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bilgeehan/Natural-Language-Processing/blob/main/stemming_and_lemmatization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming "
      ],
      "metadata": {
        "id": "2QYZwtJf3TTO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming means finding the root of the word. There are some algorithms to do this. These are Porter Stemmer Algorithm, Lancaster Algorithm and Snowball Algorithm."
      ],
      "metadata": {
        "id": "JYMtQOAh3bC4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8iDAUST13BJP"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Porter Stemmer Algorithm "
      ],
      "metadata": {
        "id": "7lsEBGyY4B_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Porter Stemmer Algorithm is mostly an algorithm for removing the suffix added to the end of the word."
      ],
      "metadata": {
        "id": "z58ZK7fA4IIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()"
      ],
      "metadata": {
        "id": "q1vYxGgn44N-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (porter.stem('stockings'))\n",
        "print (porter.stem('jumping'))\n",
        "print (porter.stem('Bunnies'))\n",
        "print (porter.stem('organization'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTZKOCWV48i9",
        "outputId": "eafbaa10-2359-41ad-fba4-f19dd0e1ab02"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stock\n",
            "jump\n",
            "bunni\n",
            "organ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lancaster"
      ],
      "metadata": {
        "id": "b_0XpNg45LlQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Lancaster algorithm is an iterative algorithm with about 120 rules."
      ],
      "metadata": {
        "id": "uyc5JbK05Qw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "lancaster = LancasterStemmer()\n"
      ],
      "metadata": {
        "id": "eUFRn2CQ53t7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (lancaster.stem('stockings'))\n",
        "print (lancaster.stem('jumping'))\n",
        "print (lancaster.stem('Bunnies'))\n",
        "print (lancaster.stem('organization'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoig7tYl56mh",
        "outputId": "a81b7dc0-ca42-42b4-bc08-609fea2c9f06"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stock\n",
            "jump\n",
            "bunny\n",
            "org\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Snowball"
      ],
      "metadata": {
        "id": "Ywkvs1oY6JC9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Snowball algorithm is a version of the Porter Stemmer algorithm developed for large datasets."
      ],
      "metadata": {
        "id": "uiLEx8Ah6LJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snow = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "pACRF7Gw6jl-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (snow.stem('stockings'))\n",
        "print (snow.stem('jumping'))\n",
        "print (snow.stem('Bunnies'))\n",
        "print (snow.stem('organization'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Aae6tqG6qei",
        "outputId": "6c0580ad-18c3-4ef9-f12b-b7d9af6e49f0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stock\n",
            "jump\n",
            "bunni\n",
            "organ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization"
      ],
      "metadata": {
        "id": "qJ62kVOe-HW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization algorithm searches and analyzes words in detailed dictionaries and finds their root."
      ],
      "metadata": {
        "id": "3WM6x_O3AgE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oddhLHSAhHN",
        "outputId": "f70e1dc4-edbc-4f4c-f1a5-9f3269950a39"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lem = WordNetLemmatizer()\n",
        "token = \"stockings\"\n",
        "result_lemma = lem.lemmatize(token)"
      ],
      "metadata": {
        "id": "6y0d2G1oAxgd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(token, \"->\", result_lemma)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pere1hLtA8ZM",
        "outputId": "4a4bb452-1d93-45a6-84dc-5a6ce42c1e57"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stockings -> stocking\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*We can want from algorithm verb or noun form of word*"
      ],
      "metadata": {
        "id": "DhSlOqA8Hw1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lem = WordNetLemmatizer()\n",
        "print(lem.lemmatize('drove'))\n",
        "print(lem.lemmatize('drove', 'v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAM4vCubHuNf",
        "outputId": "c64c81ac-9b50-4fc6-dbc6-e2b8a4ebe512"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drove\n",
            "drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Lemmatizing sentences*"
      ],
      "metadata": {
        "id": "giAGGqP-BNUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7hqIcBCBVAm",
        "outputId": "d8193f29-fba8-4c45-b2c8-269edb6ce358"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string1 = \"The girls sang louder. The bankers banked at other banks.\" \n",
        "string2 = \"These were better shoes for her feet. The grocer was stocking the shelves at the grocery\" "
      ],
      "metadata": {
        "id": "zAlxTymTBdae"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = nltk.word_tokenize(string1)\n",
        "print(\"Tokenized sentence:\")\n",
        "print(tokens) \n",
        "\n",
        "lemmatized=[]\n",
        "for w in tokens:\n",
        "  lemmatized.append(lem.lemmatize(w))\n",
        "\n",
        "strLemmatized= ' '.join(lemmatized)\n",
        "\n",
        "print(\"Lemmatized sentence:\")\n",
        "print(strLemmatized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYrcwtedBgT9",
        "outputId": "ae8e4728-28d6-49b8-ed34-64d66722243c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized sentence:\n",
            "['The', 'girls', 'sang', 'louder', '.', 'The', 'bankers', 'banked', 'at', 'other', 'banks', '.']\n",
            "Lemmatized sentence:\n",
            "The girl sang louder . The banker banked at other bank .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens2 = nltk.word_tokenize(string2)\n",
        "print(\"Tokenized sentence:\")\n",
        "print(tokens2) \n",
        "\n",
        "lemmatized_tokens2 = ' '.join([lem.lemmatize(w) for w in tokens2])\n",
        "print(\"Lemmatized sentence:\")\n",
        "print(lemmatized_tokens2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "He7sJEs6GeK2",
        "outputId": "2009b1e7-1814-4f66-831c-47b18ef47ed1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized sentence:\n",
            "['These', 'were', 'better', 'shoes', 'for', 'her', 'feet', '.', 'The', 'grocer', 'was', 'stocking', 'the', 'shelves', 'at', 'the', 'grocery']\n",
            "Lemmatized sentence:\n",
            "These were better shoe for her foot . The grocer wa stocking the shelf at the grocery\n"
          ]
        }
      ]
    }
  ]
}